{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beer_review_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yala/introML_chem/blob/master/lab2/beer_review_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA8LLAO9dchw",
        "colab_type": "text"
      },
      "source": [
        "# Beer Review Exercise!\n",
        "In this exercise, you'll extend on the tutorial from lab1 to implement neural networks to learn to analyze beer reviews. \n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT_jLzv8do9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision==0.2.0\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLJrU16dch0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMJfz2f-JC9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install wget\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_train.p\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_dev.p\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_test.p\n",
        "\n",
        "train_path = \"overall_train.p\"\n",
        "dev_path   = \"overall_dev.p\"\n",
        "test_path  = \"overall_test.p\"\n",
        "\n",
        "train_set =  pickle.load(open(train_path, 'rb'))\n",
        "dev_set =  pickle.load(open(dev_path, 'rb'))\n",
        "test_set =  pickle.load(open(test_path, 'rb'))\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    for indx, sample in enumerate(data):\n",
        "        text, label = sample['text'], sample['y']\n",
        "        text = re.sub('\\W+', ' ', text).lower().strip()\n",
        "        data[indx] = text, label\n",
        "    return data\n",
        "\n",
        "train_set = preprocess_data(train_set)\n",
        "dev_set = preprocess_data(dev_set)\n",
        "test_set =  preprocess_data(test_set)\n",
        "\n",
        "\n",
        "print(\"Num Train: {}\".format(len(train_set)))\n",
        "print(\"Num Dev: {}\".format(len(dev_set)))\n",
        "print(\"Num Test: {}\".format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4smEQXOddciA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Beer review dataset\n",
        "class BeerReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "      self.dataset = (X, Y)\n",
        "      assert X.shape[0] == len(Y)\n",
        "    def __len__(self):\n",
        "       return self.dataset[0].shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      return np.array(self.dataset[0][i].todense()[0]), self.dataset[1][i]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bj2JcZ_M1d6",
        "colab_type": "text"
      },
      "source": [
        "## Prepare your dataset (Feature Engineering)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUI6qciqMEyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract tweets and labels into 2 lists\n",
        "trainText = [t[0] for t in train_set]\n",
        "trainY = [t[1] for t in train_set]\n",
        "\n",
        "devText = [t[0] for t in dev_set]\n",
        "devY = [t[1] for t in dev_set]\n",
        "\n",
        "\n",
        "testText = [t[0] for t in test_set]\n",
        "testY = [t[1] for t in test_set]\n",
        "\n",
        "# Set that word has to appear at least 5 times to be in vocab\n",
        "min_df = 5\n",
        "max_features = 1000\n",
        "countVec = CountVectorizer(min_df = min_df, max_features = max_features )\n",
        "# Learn vocabulary from train set\n",
        "countVec.fit(trainText)\n",
        "\n",
        "# Transform list of review to matrix of bag-of-word vectors\n",
        "trainX = countVec.transform(trainText)\n",
        "devX = countVec.transform(devText)\n",
        "testX = countVec.transform(testText)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhlUt5DANWlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train = BeerReviewDataset(trainX, trainY)\n",
        "dev =   BeerReviewDataset(devX, devY)\n",
        "test =   BeerReviewDataset(testX, testY)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "dev_loader = torch.utils.data.DataLoader(dev, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vor24RSNzHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for batch in train_loader:\n",
        "  print(batch[0].shape)\n",
        "  print(batch[1].shape)\n",
        "  \n",
        "  break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFWyga62Muf9",
        "colab_type": "text"
      },
      "source": [
        "## Define your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0NnnFWgdci-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc = nn.Linear(1000, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9hvStGzdcjC",
        "colab_type": "text"
      },
      "source": [
        "## Define your training procedure\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Ht1mhhdcjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = .01\n",
        "momentum = 0.5\n",
        "\n",
        "model = Model()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAl05-rTdcjH",
        "colab_type": "text"
      },
      "source": [
        "To train our model:\n",
        "\n",
        "1) we'll randomly sample batches from our train loader\n",
        "\n",
        "2) compute our loss (using standard `cross_entropy`)\n",
        "\n",
        "3) compute our gradients (by calling `backward()` on our loss)\n",
        "\n",
        "4) update our neural network with an `optimizer.step()`, and go back to 1)\n",
        "\n",
        "I've added some extra stuff here to log our accuracy and avg loss for the epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxIj0eWsdcjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch( model, train_loader, optimizer, epoch):\n",
        "    model.train() # Set the nn.Module to train mode. \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    num_samples = len(train_loader.dataset)\n",
        "    for batch_idx, (x, target) in enumerate(train_loader): #1) get batch\n",
        "        x = x.float().squeeze(1)\n",
        "        # Reset gradient data to 0\n",
        "        optimizer.zero_grad()\n",
        "        # Get prediction for batch\n",
        "        output = model(x)\n",
        "        # 2) Compute loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        #3) Do backprop\n",
        "        loss.backward()\n",
        "        #4) Update model\n",
        "        optimizer.step()\n",
        "        \n",
        "        ## Do book-keeping to track accuracy and avg loss\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        total_loss += loss.detach() # Don't keep computation graph \n",
        "\n",
        "    print('Train Epoch: {} \\tLoss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            epoch, total_loss / num_samples, \n",
        "            correct, \n",
        "            num_samples,\n",
        "            100. * correct / num_samples))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGZnlI58dcjN",
        "colab_type": "text"
      },
      "source": [
        "## Define our evaluation loop\n",
        "Similar to above, we'll also loop through our dev or test set, and compute our loss and accuracy. \n",
        "This lets us see how well our model is generalizing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPzqSuY3dcjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_epoch(model, test_loader, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data = data.float().squeeze(1)\n",
        "        target = target.long()\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        name,\n",
        "        test_loss, \n",
        "        correct, \n",
        "        len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp9H5tWEdcjR",
        "colab_type": "text"
      },
      "source": [
        "## Train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVpU_N0idcjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_loader, optimizer, epoch)\n",
        "    eval_epoch(model,  dev_loader, \"Dev\")\n",
        "    print(\"---\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNWqKY72dcjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_epoch(model,  test_loader, \"Test\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}