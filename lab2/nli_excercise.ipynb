{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nli_solution.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yala/deeplearning_bootcamp/blob/master/lab2/nli_excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yXAB2YFIhjZ7"
      },
      "source": [
        "# Build a Natural Language Inference Classifier (with PyTorch)\n",
        "\n",
        "Natural language inference is the task of determining whether or not a given statement (the \"hypothesis\") is entailed by another given statement (the \"premise\").\n",
        "\n",
        "The hypothesis is true (entailment) if it is entailed, it is false (contradiction) if it is not entailed, and it is undetermined (neutral) if it is neither true nor false.\n",
        "\n",
        "An example is:\n",
        "\n",
        "| Premise | Label | Hypothesis |\n",
        "| ---  | --- | --- |\n",
        "|The Golden State Warriors scored 100 points last night.| Entailment | Someone scored a basket in the game. |\n",
        "|The Golden State Warriors scored 100 points last night. | Neutral | The Warriors won the game. |\n",
        "| The Golden State Warriors scored 100 points last night. | Contradiction | The Warriors struggled to make baskets. |\n",
        "\n",
        "\n",
        "## Dataset\n",
        "\n",
        "For this exercise we'll be using a portion of the [MNLI](https://arxiv.org/abs/1704.05426) dataset --- a dataset for natural language inference that spans multiple genres and writing styles. To keep things simple, we will only be dealing with the \"Entailment\" and \"Contradiction\" classes --- making it a binary classification task.\n",
        "\n",
        "The data is provided to you as a list of entries, where each `entry` has the following structure:\n",
        "\n",
        "```\n",
        "example.x1 = [\"the\", \"tokenized\", \"premise\"]\n",
        "example.x2 = [\"the\", \"tokenized\", \"hypothesis\"]\n",
        "example.y = 0 or 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FYUFWtjssSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data.\n",
        "!wget https://people.csail.mit.edu/fisch/assets/data/bootcamp/nli/train.txt\n",
        "!wget https://people.csail.mit.edu/fisch/assets/data/bootcamp/nli/valid.txt\n",
        "!wget https://people.csail.mit.edu/fisch/assets/data/bootcamp/nli/test.txt\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "LABELS = [\"contradiction\", \"entailment\"]\n",
        "\n",
        "Example = collections.namedtuple(\"Entry\", [\"x1\", \"x2\", \"y\"])\n",
        "\n",
        "def load_data(filename):\n",
        "  examples = []\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      fields = json.loads(line)\n",
        "      x1 = fields[\"x1\"]\n",
        "      x2 = fields[\"x2\"]\n",
        "      if fields[\"y\"] not in LABELS:\n",
        "        continue\n",
        "      y = LABELS.index(fields[\"y\"])\n",
        "      examples.append(Example(x1, x2, y))\n",
        "  return examples\n",
        "\n",
        "train_examples = load_data(\"train.txt\")\n",
        "valid_examples = load_data(\"valid.txt\")\n",
        "test_examples = load_data(\"test.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3qofsHBuHAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Set vocab using train text.\n",
        "min_df = 5\n",
        "max_features = 3000\n",
        "vectorizer = CountVectorizer(min_df=min_df, max_features=max_features)\n",
        "vectorizer.fit([\" \".join(ex.x1) for ex in train_examples] +\n",
        "               [\" \".join(ex.x2) for ex in train_examples])\n",
        "\n",
        "\n",
        "def prepare_dataset(examples, batch_size=4):\n",
        "  # Convert all strings to indices.\n",
        "  x1 = vectorizer.transform([\" \".join(ex.x1) for ex in examples]).toarray()\n",
        "  x2 = vectorizer.transform([\" \".join(ex.x2) for ex in examples]).toarray()\n",
        "  \n",
        "  # Isolate the labels.\n",
        "  y = np.array([ex.y for ex in examples])\n",
        "\n",
        "  # Convert to torch tensors.\n",
        "  x1 = torch.from_numpy(x1)\n",
        "  x2 = torch.from_numpy(x2)\n",
        "  y = torch.from_numpy(y)\n",
        "\n",
        "  # Wrap in a pytorch tensor dataset.\n",
        "  dataset = torch.utils.data.TensorDataset(x1, x2, y)\n",
        "\n",
        "  # Load the dataset with a data loader.\n",
        "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return loader\n",
        "\n",
        "\n",
        "train_data = prepare_dataset(train_examples)\n",
        "valid_data = prepare_dataset(valid_examples)\n",
        "test_data = prepare_dataset(test_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhby7X9149Dy",
        "colab_type": "text"
      },
      "source": [
        "## Boilerplate training code\n",
        "\n",
        "The standard functions that are helpful to train your networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4t_dG-b45j-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch( model, train_loader, optimizer, epoch):\n",
        "  model.train() # Set the nn.Module to train mode. \n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  num_samples = len(train_loader.dataset)\n",
        "  for batch_idx, (x1, x2, target) in enumerate(train_loader): #1) get batch\n",
        "    x1 = x1.float()\n",
        "    x2 = x2.float()\n",
        "    target = target.float()\n",
        "    # Reset gradient data to 0\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "    # Get prediction for batch\n",
        "    output = model(x1, x2).squeeze(1)\n",
        "  \n",
        "    # 2) Compute loss\n",
        "    # YOUR CODE HERE\n",
        "  \n",
        "    #3) Do backprop\n",
        "    loss.backward()\n",
        "  \n",
        "    #4) Update model\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.detach() # Don't keep computation graph \n",
        "\n",
        "  print('Train Epoch: {} \\tXENT: {:.4f})\\n'.format(\n",
        "        epoch, total_loss / num_samples))\n",
        "  \n",
        "\n",
        "\n",
        "def eval_epoch(model, test_loader, name):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0.0\n",
        "  for x1, x2, target in test_loader:\n",
        "    x1 = x1.float()\n",
        "    x2 = x2.float()\n",
        "    target = target.float()\n",
        "    output = model(x1, x2).squeeze(-1)\n",
        "    # YOUR CODE HERE. Get accuracy and loss!\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  correct /= len(test_loader.dataset)\n",
        "  print('\\n{} set: Average XENT: {:.4f}\\n'.format(name, test_loss))\n",
        "  print('\\n{} set: Average Acc: {:.4f}\\n'.format(name, correct))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODRF2d_Mx8xT",
        "colab_type": "text"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Try different model choices!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtmO8CH56JKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Training settings\n",
        "epochs = 10\n",
        "lr = .01\n",
        "momentum = 0.5\n",
        "hidden_dim = 100\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    # YOUR CODE HERE\n",
        "    pass  \n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "model = Model()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPuovaVi9lEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_epoch(model, train_data, optimizer, epoch)\n",
        "    eval_epoch(model,  valid_data, \"Dev\")\n",
        "    print(\"---\")\n",
        "eval_epoch(model,  test_data, \"Test\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}