{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yala/introML_chem/blob/master/lab1/sentiment_analysis_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNic6xm-HFj-",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Machine Learning Packages\n",
        "\n",
        "In this tutorial, we'll take you through developing models to classify text in `sklearn` from start to finish. We'll go through preprocessing, feature engineering, and experimentation. \n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAhHi9YmHFkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model  import PassiveAggressiveClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLaCOU61HFkK",
        "colab_type": "text"
      },
      "source": [
        "# The Task: Beer Sentiment Analysis\n",
        "\n",
        "Given long and detailed beer reviews, we want to predict if the reviewed ranked it as an bad, okay or good.\n",
        "\n",
        "\n",
        "## Step 1: Preprocessing the data\n",
        "To start off, we're going to load the data from some pickle files and do some simple preprocessing. We'll throw away non-alphanumeric characters and lowercase everything.\n",
        "\n",
        "i.e\n",
        "```\"Best Beer ever!!!\" -> \"best beer ever\"```\n",
        "\n",
        "The sanity check the data, we'll look at a few examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fOECW2YHFkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install wget\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_train.p\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_dev.p\n",
        "!wget https://raw.githubusercontent.com/yala/MLCodeLab/master/lab1/data/beer/overall_test.p\n",
        "\n",
        "train_path = \"overall_train.p\"\n",
        "dev_path   = \"overall_dev.p\"\n",
        "test_path  = \"overall_test.p\"\n",
        "\n",
        "train_set =  pickle.load(open(train_path, 'rb'))\n",
        "dev_set =  pickle.load(open(dev_path, 'rb'))\n",
        "test_set =  pickle.load(open(test_path, 'rb'))\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    for indx, sample in enumerate(data):\n",
        "        text, label = sample['text'], sample['y']\n",
        "        text = re.sub('\\W+', ' ', text).lower().strip()\n",
        "        data[indx] = text, label\n",
        "    return data\n",
        "\n",
        "train_set = preprocess_data(train_set)\n",
        "dev_set = preprocess_data(dev_set)\n",
        "test_set =  preprocess_data(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqsdTQ0QHFkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Num Train: {}\".format(len(train_set)))\n",
        "print(\"Num Dev: {}\".format(len(dev_set)))\n",
        "print(\"Num Test: {}\".format(len(test_set)))\n",
        "print(\"Example Reviews:\")\n",
        "print(train_set[0])\n",
        "print()\n",
        "print(train_set[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLUarFjsHFkb",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Feature Engineering \n",
        "\n",
        "How do we represent a review? We're going to use a simple bag of words representation. Meaning we'll represent each review as a vector, and the whole set of reviews as a large matrix.\n",
        "\n",
        "For example, consider our vocabulary is ```[best, ever, beer, cat, good, dog]```.\n",
        "The bag of words representation for:\n",
        "```\"best beer ever\"``` is ```[1, 1, 1, 0, 0, 0]```\n",
        "Where one indicates that the vocab words did appear and 0 indicates the words that did not. S\n",
        "\n",
        "With sklearn, we can do this very easily with ```sklearn.feature_extraction.text.CountVectorizer```\n",
        "\n",
        "<img src=\"https://github.com/yala/MLCodeLab/blob/master/lab1/vectorizer.png?raw=true\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1CzWuiuHFkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract tweets and labels into 2 lists\n",
        "trainText = [t[0] for t in train_set]\n",
        "trainY = [t[1] for t in train_set]\n",
        "\n",
        "devText = [t[0] for t in dev_set]\n",
        "devY = [t[1] for t in dev_set]\n",
        "\n",
        "\n",
        "testText = [t[0] for t in test_set]\n",
        "testY = [t[1] for t in test_set]\n",
        "\n",
        "# Set that word has to appear at least 5 times to be in vocab\n",
        "min_df = 5\n",
        "max_features = 1000\n",
        "countVec = CountVectorizer(min_df = min_df, max_features = max_features )\n",
        "# Learn vocabulary from train set\n",
        "countVec.fit(trainText)\n",
        "\n",
        "# Transform list of review to matrix of bag-of-word vectors\n",
        "trainX = countVec.transform(trainText)\n",
        "devX = countVec.transform(devText)\n",
        "testX = countVec.transform(testText)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnpeX3drHFkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Shape of Train X {}\\n\".format(trainX.shape))\n",
        "print(\"Sample of the vocab:\\n {}\".format(np.random.choice(countVec.get_feature_names(), 20)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZeK-LcYHFko",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Pick a model and experiment\n",
        "\n",
        "Here we'll explore various types of linear models, namely Logistic Regression, Passive Aggressive, and Perceptron. It's very straight-forward\n",
        "to fit a new classifier and get preliminary results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFgurZU1HFkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()\n",
        "passAgg    = PassiveAggressiveClassifier()\n",
        "perceptron = Perceptron()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aHeIHpbHFkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(trainX, trainY)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Train:\", lr.score(trainX, trainY))\n",
        "print(\"Logistic Regression Dev:\", lr.score(devX, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H37jOG4HFk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "passAgg.fit(trainX, trainY) \n",
        "print(\"Passive Aggressive Train:\", passAgg.score(trainX, trainY))\n",
        "print(\"Passive Aggressive Dev:\", passAgg.score(devX, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9LpPv8-HFlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perceptron.fit(trainX, trainY) \n",
        "print(\"Perceptron Train:\", perceptron.score(trainX, trainY))\n",
        "print(\"Perceptron Dev:\", perceptron.score(devX, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jz04-8dHFlI",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Analysis, Debugging the Model\n",
        "To understand how to make the model better, it's important understand what the model is learning, and what it's getting wrong.\n",
        "\n",
        "To do this, we can inspect the highest weighted features of our best LR model and look at some examples the model got wrong on the development set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2DuosInHFlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(trainX, trainY)\n",
        "print(\"Logistic Regression Train:\", lr.score(trainX, trainY))\n",
        "print(\"Logistic Regression Dev:\", lr.score(devX, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsyo-PLwHFlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Intepreting LR\")\n",
        "for label in range(3):\n",
        "    coefs = lr.coef_[label]\n",
        "    vocab = np.array(countVec.get_feature_names())\n",
        "    num_features = 10\n",
        "\n",
        "    top = np.argpartition(coefs, -num_features)[-num_features:]\n",
        "    # Sort top\n",
        "    top = top[np.argsort(coefs[top])]\n",
        "    s_coef = coefs[top]\n",
        "    scored_vocab = list(zip(vocab[top], s_coef))\n",
        "    print(\"Top weighted features for label {}:\\n \\n {}\\n -- \\n\".format(label, scored_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGk01kBHFlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Find erronous dev errors\n",
        "devPred = lr.predict(devX)\n",
        "errors = []\n",
        "for indx in range(len(devText)):\n",
        "    if devPred[indx] != devY[indx]:\n",
        "        error = \"Review: \\n {} \\n Predicted: {} \\n Correct: {} \\n ---\".format(\n",
        "            devText[indx],\n",
        "            devPred[indx],\n",
        "            devY[indx])\n",
        "        errors.append(error)\n",
        "\n",
        "np.random.seed(2)\n",
        "print(\"Random dev error: \\n {} \\n \\n {} \\n \\n{}\".format(\n",
        "        np.random.choice(errors,1),\n",
        "        np.random.choice(errors,1),\n",
        "        np.random.choice(errors,1))\n",
        "     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pGVb1i7HFlb",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Play with regularization\n",
        "\n",
        "We can see that LogisticRegression so far works the best so far, but it is greatly over fitting. Meaning that it does much better on train than development. A common strategy to dealing with this is adding an extra penalty for model complexity, like the square sum of the model weights. We call this idea regularization. \n",
        "\n",
        "In sklearn, it is very easy to test out various regularization amounts and tune the model. The smaller the parameter `C`, the stronger the regularization cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-edhCWbOHFld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression(C=.5)\n",
        "lr.fit(trainX, trainY)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Train:\", lr.score(trainX, trainY))\n",
        "print(\"Logistic Regression Dev:\", lr.score(devX, devY))\n",
        "print(\"--\")\n",
        "\n",
        "lr = LogisticRegression(C=.1)\n",
        "lr.fit(trainX, trainY)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Train:\", lr.score(trainX, trainY))\n",
        "print(\"Logistic Regression Dev:\", lr.score(devX, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp6127uOHFlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression(C=.01)\n",
        "lr.fit(trainX, trainY)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Train:\", lr.score(trainX, trainY))\n",
        "print(\"Logistic Regression Dev:\", lr.score(devX, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jipGDZXiHFlq",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Adding in Ngrams\n",
        "\n",
        "How does our model distinguish between the sentiment phrase that says:\n",
        "```\"great flavor and too bad there isn't more.\"```\n",
        "versus\n",
        "```\"bad flavor and too great there isn't more.\"```\n",
        "\n",
        "In our bag of words model, both have the same vector. In order to capture some of these ordering depency, we generalize the bag-of-words model to take \"n-grams\" of words that occur in the training set. a \"bi-gram\" is a pair of words, \"tri-gram\" triple, etc.\n",
        "\n",
        "Let see how this imporves our model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stptfRWrHFls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set that word has to appear at least 5 times to be in vocab\n",
        "min_df = 5\n",
        "ngram_range = (1,3)\n",
        "max_features = 5000\n",
        "countVecNgram = CountVectorizer(min_df = min_df, ngram_range = ngram_range, max_features=max_features)\n",
        "# Learn vocabulary from train set\n",
        "countVecNgram.fit(trainText)\n",
        "\n",
        "# Transform list of review to matrix of bag-of-word vectors\n",
        "trainXNgram = countVecNgram.transform(trainText)\n",
        "devXNgram = countVecNgram.transform(devText)\n",
        "testXNgram = countVecNgram.transform(testText)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xCmdRYHFly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrNgram = LogisticRegression(C=1)\n",
        "lrNgram.fit(trainXNgram, trainY)\n",
        "print(\"Logistic Regression Train:\", lrNgram.score(trainXNgram, trainY))\n",
        "print(\"Logistic Regression Dev:\", lrNgram.score(devXNgram, devY))\n",
        "print(\"--\")\n",
        "\n",
        "lrNgram = LogisticRegression(C=.5)\n",
        "lrNgram.fit(trainXNgram, trainY)\n",
        "print(\"Logistic Regression Train:\", lrNgram.score(trainXNgram, trainY))\n",
        "print(\"Logistic Regression Dev:\", lrNgram.score(devXNgram, devY))\n",
        "print(\"--\")\n",
        "\n",
        "lrNgram = LogisticRegression(C=.1)\n",
        "lrNgram.fit(trainXNgram, trainY)\n",
        "print(\"Logistic Regression Train:\", lrNgram.score(trainXNgram, trainY))\n",
        "print(\"Logistic Regression Dev:\", lrNgram.score(devXNgram, devY))\n",
        "print(\"--\")\n",
        "\n",
        "lrNgram = LogisticRegression(C=.01)\n",
        "lrNgram.fit(trainXNgram, trainY)\n",
        "print(\"Logistic Regression Train:\", lrNgram.score(trainXNgram, trainY))\n",
        "print(\"Logistic Regression Dev:\", lrNgram.score(devXNgram, devY))\n",
        "print(\"--\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mp-B3tVHFl7",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Take best model, and report results on Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKz302S-HFl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Logistic Regression Test:\", lrNgram.score(testXNgram, testY))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR7IN8zSHFmF",
        "colab_type": "text"
      },
      "source": [
        "## Next Step: Doing it on your own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr5mw4QrHFmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}