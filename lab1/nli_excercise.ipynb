{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nli_excercise.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yala/deeplearning_bootcamp/blob/master/lab1/nli_excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yXAB2YFIhjZ7"
      },
      "source": [
        "# Build a Natural Language Inference Classifier\n",
        "\n",
        "Natural language inference is the task of determining whether or not a given statement (the \"hypothesis\") is entailed by another given statement (the \"premise\").\n",
        "\n",
        "The hypothesis is true (entailment) if it is entailed, it is false (contradiction) if it is not entailed, and it is undetermined (neutral) if it is neither true nor false.\n",
        "\n",
        "An example is:\n",
        "\n",
        "| Premise | Label | Hypothesis |\n",
        "| ---  | --- | --- |\n",
        "|The Golden State Warriors scored 100 points last night.| Entailment | Someone scored a basket in the game. |\n",
        "|The Golden State Warriors scored 100 points last night. | Neutral | The Warriors won the game. |\n",
        "| The Golden State Warriors scored 100 points last night. | Contradiction | The Warriors struggled to make baskets. |\n",
        "\n",
        "\n",
        "## Dataset\n",
        "\n",
        "For this exercise we'll be using a portion of the [MNLI](https://arxiv.org/abs/1704.05426) dataset --- a dataset for natural language inference that spans multiple genres and writing styles. To keep things simple, we will only be dealing with the \"Entailment\" and \"Contradiction\" classes --- making it a binary classification task.\n",
        "\n",
        "The data is provided to you as a list of entries, where each `entry` has the following structure:\n",
        "\n",
        "```\n",
        "example.x1 = [\"the\", \"tokenized\", \"premise\"]\n",
        "example.x2 = [\"the\", \"tokenized\", \"hypothesis\"]\n",
        "example.y = 0 or 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FYUFWtjssSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data.\n",
        "!wget https://people.csail.mit.edu/fisch/assets/data/bootcamp/nli/train.txt\n",
        "!wget https://people.csail.mit.edu/fisch/assets/data/bootcamp/nli/valid.txt\n",
        "!wget https://people.csail.mit.edu/fisch/assets/data/bootcamp/nli/test.txt\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "LABELS = [\"contradiction\", \"entailment\"]\n",
        "\n",
        "Example = collections.namedtuple(\"Entry\", [\"x1\", \"x2\", \"y\"])\n",
        "\n",
        "def load_data(filename):\n",
        "  examples = []\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      fields = json.loads(line)\n",
        "      x1 = fields[\"x1\"]\n",
        "      x2 = fields[\"x2\"]\n",
        "      if fields[\"y\"] not in LABELS:\n",
        "        continue\n",
        "      y = LABELS.index(fields[\"y\"])\n",
        "      examples.append(Example(x1, x2, y))\n",
        "  return examples\n",
        "\n",
        "train_examples = load_data(\"train.txt\")\n",
        "valid_examples = load_data(\"valid.txt\")\n",
        "test_examples = load_data(\"test.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRrG0gKopRxi",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "As you can see from the example, this task takes **two** inputs $x_1$ and $x_2$. We'll experiment with some basic featurization options.\n",
        "\n",
        "Feel free to explore additional feature engineering approaches if you have time!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmFkLOM8ygfJ",
        "colab_type": "text"
      },
      "source": [
        "### Majority baseline\n",
        "\n",
        "It's always good to start simple when approaching new task. Na√Øve baselines are often good at uncovering biases in the data you might not have noticed otherwise.\n",
        "\n",
        "One to start out with is the majority baseline. What is the prior for entailment? In this model simply ignore the input and use the most common class, always.\n",
        "\n",
        "Using the [DummyClassifier](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators) from `sklearn`, create a majority baseline and record the accuracy. See how much higher you can get in the next sections!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHMMdjJfzGNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: YOUR CODE HERE\n",
        "\n",
        "majority_baseline = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQoLpMZ4uDcN",
        "colab_type": "text"
      },
      "source": [
        "### Hypothesis- and premise-only baselines\n",
        "\n",
        "Two other simple baselines are to try to classify the data using just the hypothesis (and no premise) and just the premise (and no hypothesis). You may use a bag-of-words representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3qofsHBuHAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: YOUR CODE HERE\n",
        "\n",
        "hypothesis_only = ...\n",
        "\n",
        "premise_only = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxpfBG9NpN8w",
        "colab_type": "text"
      },
      "source": [
        "### Independent features\n",
        "\n",
        "Let's now create a featurization that includes both $x_1$ and $x_2$. A simple one to begin with is the concatenation of their bag-of-words vectors: $[\\texttt{BoW}(x_1); \\texttt{BoW}(x_2)]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X9aaZHqvQJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: YOUR CODE HERE\n",
        "\n",
        "concatenated = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRkpgdn2vhDl",
        "colab_type": "text"
      },
      "source": [
        "### Interaction features\n",
        "\n",
        "The concatenated features don't capture any interactions between the premise and hypothesis. We can also try to add some more features by considering the number of shared terms in both sentences: $\\min(x_1, x_2)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1EjeJGPxevR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: YOUR CODE HERE\n",
        "\n",
        "overlap = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODRF2d_Mx8xT",
        "colab_type": "text"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Using your featurizations as inputs, experiment with different modeling choices using `sklearn`.\n",
        "\n",
        "Try a logistic regression model first, with different regularization stragies. Then you may move on to non-linear classifiers, such as decision trees. Try to get as high an accuracy that you can!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtmO8CH56JKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}